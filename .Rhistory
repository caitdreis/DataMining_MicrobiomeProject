lines(mpg, fit8a, lwd = 2, col ='green', lty = 3 )
library(ISLR)
library(e1071)
library(ROCR)
set.seed(17)
x=matrix(rnorm(200*2), ncol=2)
x[1:100,]=x[1:100,]+2
x[101:150,]=x[101:150,]-2
y=c(rep(1,150),rep(2,50))
dat=data.frame(x=x,y=as.factor(y))
set.seed(17)
x=matrix(rnorm(100*2), ncol=2)
x[1:50,]=x[1:50,]+3
x[51:100,]=x[51:100,]-3
y=c(rep(1,75),rep(2,25))
data4=data.frame(x=x,y=as.factor(y))
set.seed(17)
x=matrix(rnorm(100*2), ncol=2)
x[1:50,]=x[1:50,]+3
x[51:100,]=x[51:100,]-3
y=c(rep(1,75),rep(2,25))
data4=data.frame(x=x,y=as.factor(y))
set.seed(17)
x=matrix(rnorm(100*2), ncol=2)
x[1:50,]=x[1:50,]+2
x[51:100,]=x[51:100,]-2
y=c(rep(1,75),rep(2,25))
data4=data.frame(x=x,y=as.factor(y))
set.seed(17)
x=matrix(rnorm(100*2), ncol=1)
x[1:50,]=x[1:50,]+2
x[51:100,]=x[51:100,]-2
y=c(rep(1,75),rep(2,25))
data4=data.frame(x=x,y=as.factor(y))
set.seed(17)
x=matrix(rnorm(100), ncol=1)
x[1:50,]=x[1:50,]+2
x[51:100,]=x[51:100,]-2
y=c(rep(1,75),rep(2,25))
data4=data.frame(x=x,y=as.factor(y))
plot(data4)
set.seed(17)
x=matrix(rnorm(100), ncol=2)
x[1:50,]=x[1:50,]+2
x[51:100,]=x[51:100,]-2
y=c(rep(1,75),rep(2,25))
data4=data.frame(x=x,y=as.factor(y))
set.seed(17)
x=matrix(rnorm(100*2), ncol=2)
x[1:50,]=x[1:50,]+2
x[51:100,]=x[51:100,]-2
y=c(rep(1,75),rep(2,25))
data4=data.frame(x=x,y=as.factor(y))
set.seed(1)
x=matrix(rnorm(200*2), ncol=2)
x[1:100,]=x[1:100,]+2
x[101:150,]=x[101:150,]-2
y=c(rep(1,150),rep(2,50))
dat=data.frame(x=x,y=as.factor(y))
plot(data4)
lin.svm=svm(y~., data=data4, kernel="linear", cost=10, scale=FALSE)
plot(lin.svm , data4)
summary(lin.svm)
lin.svm$index
x4test=matrix(rnorm(20*2), ncol=2)
x[1:10,]=x[1:10,]+2
x[11:20,]=x[11:20,]-2
y=c(rep(1,7),rep(2,3))
test4data=data.frame(x=x4test, y=as.factor(y4test))
y4test=c(rep(1,7),rep(2,3))
test4data=data.frame(x=x4test, y=as.factor(y4test))
ypred4 =predict(lin.svm,test4data)
table(predict=ypred4, truth=test4data$y)
table(true=data4[,"y"], pred=predict(radial.fit, newdata=x4test))
radial.fit=svm(y~ ., data=data4, kernel="radial",   gamma=1, cost=1)
plot(radial.fit, data4)
table(true=data4[,"y"], pred=predict(radial.fit, newdata=x4test))
table(true=x4test, pred=predict(radial.fit, newdata=x4test))
radial.fit=svm(y~ ., data=data4, kernel="radial",   gamma=1, cost=1)
plot(radial.fit, data4)
table(true=x4test$y, pred=predict(radial.fit, newdata=x4test))
ypred4b = predict(radial.fit, newdata=x4test)
table(predict=ypred4b, truth = truth=test4data$y)
table(predict=ypred4b, truth = test4data$y)
radial.fit=svm(y~ ., data=data4, kernel="radial",   gamma=4, cost=1)
plot(radial.fit, data4)
ypred4b = predict(radial.fit, newdata=x4test)
table(predict=ypred4b, truth = test4data$y)
radial.fit=svm(y~ ., data=data4, kernel="radial",   gamma=4, cost=.5)
plot(radial.fit, data4)
ypred4b = predict(radial.fit, newdata=x4test)
table(predict=ypred4b, truth = test4data$y)
radial.fit=svm(y~ ., data=data4, kernel="radial",   gamma=4, cost=5)
plot(radial.fit, data4)
ypred4b = predict(radial.fit, newdata=x4test)
table(predict=ypred4b, truth = test4data$y)
radial.fit=svm(y~ ., data=data4, kernel="radial",   gamma=2, cost=5)
plot(radial.fit, data4)
ypred4b = predict(radial.fit, newdata=x4test)
table(predict=ypred4b, truth = test4data$y)
radial.fit=svm(y~ ., data=data4, kernel="radial",   gamma=5, cost=5)
plot(radial.fit, data4)
ypred4b = predict(radial.fit, newdata=x4test)
table(predict=ypred4b, truth = test4data$y)
radial.fit=svm(y~ ., data=data4, kernel="radial",   gamma=4, cost=5)
plot(radial.fit, data4)
ypred4b = predict(radial.fit, newdata=x4test)
table(predict=ypred4b, truth = test4data$y)
radial.fit=svm(y~ ., data=data4, kernel="radial",   gamma=4, cost=10)
plot(radial.fit, data4)
ypred4b = predict(radial.fit, newdata=x4test)
table(predict=ypred4b, truth = test4data$y)
x1=runif(500)-0.5
x2=runif(500)-0.5
y=1*(x1^2-x2^2 > 0)
data5 = data.frame(x=x, y=as.factor(y))
data5 = data.frame(x=x1,x2, y=as.factor(y))
plot(data5)
library(ISLR)
library(RColorBrewer)
cols<-brewer.pal(n=2,name="Set1")
cols<-('red''green')
cols<-('red''green')
plot(x1,x2 col = 'red')
plot(x1,x2)
library(ggplot2)
qplot(x1, x2, data = data5, colour = color)
qplot(x1, x2, data = data5, color = color)
qplot(x1, x2, data = data5, color = y)
qplot(x1, x2, data = data5, color = x1)
qplot(x1, x2, data = data5, color = x1,x2)
qplot(x1, x2, data = data5, color = x2)
qplot(x1, x2, data = data5, color = y)
log-fit = glm(y~., data = data5, family = binomial)
log.fit = glm(y~., data = data5, family = binomial)
predict(log.fit, data = data5)
logpred = predict(log.fit, data = data5)
qplot(logpred)
qplot(logpred, color=y)
qplot(logpred, bins = 30, color=y)
logpred = predict(log.fit, data = data5, type = "response")
qplot(logpred, color=y)
logprob= predict(log.fit, data = data5, type = "response")
log.pred = ifelse(logprob > 0.55, 1, 0)
qplot(logpred, color=y)
qplot(log.pred, color=y)
plot(data.pos$x1, data.pos$x2, col = "blue", xlab = "X1", ylab = "X2")
points(data.neg$x1, data.neg$x2, col = "red", pch = 4)
data.pos = data5[log.pred == 1, ]
data.neg = data5[log.pred == 0, ]
plot(data.pos$x1, data.pos$x2, col = "blue", xlab = "X1", ylab = "X2")
logprob= predict(log.fit, data = data5, type = "response")
log.pred = ifelse(logprob > 0.55, 1, 0)
data.pos = data5[log.pred == 1, ]
data.neg = data5[log.pred == 0, ]
plot(data.pos$x1, data.pos$x2, col = "blue", xlab = "X1", ylab = "X2")
x1=runif(500)-0.5
x2=runif(500)-0.5
x1=runif(500)-0.5
x2=runif(500)-0.5
y=1*(x1^2-x2^2 > 0)
data5 = data.frame(x=x1,x2, y=as.factor(y))
qplot(x1, x2, data = data5, color = y)
log.fit = glm(y~., data = data5, family = binomial)
logprob= predict(log.fit, data = data5, type = "response")
log.pred = ifelse(logprob > 0.55, 1, 0)
data.pos = data5[log.pred == 1, ]
data.neg = data5[log.pred == 0, ]
plot(data.pos$x1, data.pos$x2, col = "blue", xlab = "X1", ylab = "X2")
logprob= predict(log.fit, data = data5, type = "response")
log.pred = ifelse(logprob > 0.50, 1, 0)
data.pos = data5[log.pred == 1, ]
data.neg = data5[log.pred == 0, ]
plot(data.pos$x1, data.pos$x2, col = "blue", xlab = "X1", ylab = "X2")
log.poly = glm(y~poly(x1,3) + x2, data = data5, family = binomial)
logprob= predict(log.fit, data = data5, type = "response")
log.pred = ifelse(logprob > 0.50, 1, 0)
data.pos = data5[log.pred == 1, ]
data.neg = data5[log.pred == 0, ]
plot(data.pos$x1, data.pos$x2, col = "blue", xlab = "X1", ylab = "X2")
svm.fit = svm(as.factor(y) ~ x1 + x2, data = data5, kernel = "linear", cost = 0.1)
svm.pred = predict(svm.fit, data5)
data.pos = data[svm.pred == 1, ]
plot(svm.pred, col = "blue", xlab = "X1", ylab = "X2", pch = "+")
set.seed(17)
x=matrix(rnorm(20*2), ncol=2)
y=c(rep(-1,10), rep(1,10))
x[y==1,]=x[y==1,] + 1
plot(x, col=(3-y))
set.seed(17)
x=matrix(rnorm(20*2), ncol=2)
y=c(rep(-1,10), rep(1,10))
x[y==1,]=x[y==1,] + 2
plot(x, col=(3-y))
set.seed(17)
x=matrix(rnorm(20*2), ncol=2)
y=c(rep(-1,10), rep(1,10))
x[y==1,]=x[y==1,] + 1.5
plot(x, col=(3-y))
set.seed(17)
x=matrix(rnorm(40*2), ncol=2)
y=c(rep(-1,10), rep(1,10))
x[y==1,]=x[y==1,] + 1.5
plot(x, col=(3-y))
set.seed(17)
x=matrix(rnorm(40*2), ncol=2)
y=c(rep(-1,10), rep(1,10))
x[y==1,]=x[y==1,] + 2
plot(x, col=(3-y))
data6 = data.frame(x=x, y=as.factor(y))
set.seed(7)
tune.out6=tune(svm,y~ .,data=data6,kernel="linear",ranges=list(cost=c(0.001, 0.01, 0.1, 1,5,10,100)))
summary(tune.out6)
set.seed(15)
x=matrix(rnorm(40*2), ncol=2)
y=c(rep(-1,10), rep(1,10))
x[y==1,]=x[y==1,] + 2
plot(x, col=(3-y))
data6test = data.frame(x=x, y=as.factor(y))
costs = c(0.01, 0.1, 1, 5, 10, 100, 1000, 10000)
test.errors = rep(NA, 8)
for (i in 1:length(costs)) {
svm6.fit = svm(as.factor(y) ~ ., data = data6, kernel = "linear", cost = costs[i])
svm6.predict = predict(svm6.fit, data6test)
test.errors[i] = sum(svm6.predict != data6.test$y)
}
svm6.fit = svm(as.factor(y) ~ ., data = data6, kernel = "linear", cost = costs[i])
data.frame(cost = all.costs, `test misclass` = test.errors)
data.frame(cost = costs, `test misclass` = test.errors)
gas.med = median(Auto$mpg)
bi.gas = ifelse(Auto$mpg > gas.med, 1, 0)
Auto$mpglevel = as.factor(bi.gas)
set.seed(22)
tune.out=tune(svm,mpglevel~ .,data=Auto,kernel="linear",ranges=list(cost=c(0.001, 0.01, 0.1, 1,5,10,100)))
summary(tune.out)
set.seed(22)
tune.out = tune(svm, mpglevel ~ ., data = Auto, kernel = "polynomial", ranges = list(cost = c(0.1,  1, 5, 10), degree = c(2, 3, 4)))
summary(tune.out)
set.seed(22)
tune.out = tune(svm, mpglevel ~ ., data = Auto, kernel = "radial", ranges = list(cost = c(0.1,  1, 5, 10), gamma = c(0.01, 0.1, 1, 5, 10, 100)))
summary(tune.out)
svm.linear = svm(mpglevel ~ ., data = Auto, kernel = "linear", cost = 1)
svm.poly = svm(mpglevel ~ ., data = Auto, kernel = "polynomial", cost = 10,  degree = 2)
svm.radial = svm(mpglevel ~ ., data = Auto, kernel = "radial", cost = 10, gamma = 0.01)
plotpairs = function(fit) {
for (name in names(Auto)[!(names(Auto) %in% c("mpg", "mpglevel", "name"))]) {
plot(fit, Auto, as.formula(paste("mpg~", name, sep = "")))
}
}
plotpairs(svm.linear)
x1 = c(3, 2, 4, 1, 2, 4, 4)
x2 = c(4, 2, 4, 4, 1, 3, 1)
colors = c("red", "red", "red", "red", "blue", "blue", "blue")
plot(x1, x2, col = colors, xlim = c(0, 5), ylim = c(0, 5))
plot(x1, x2, col = colors, xlim = c(0, 5), ylim = c(0, 5))
abline(-0.5, 1)
plot(x1, x2, col = colors, xlim = c(0, 5), ylim = c(0, 5))
abline(-0.5, 1)
abline(-1, 1, lty = 2)
abline(0, 1, lty = 2)
plot(x1, x2, col = colors, xlim = c(0, 5), ylim = c(0, 5))
abline(-0.5, 1)
arrows(2, 1, 2, 1.5)
arrows(2, 2, 2, 1.5)
arrows(4, 4, 4, 3.5)
arrows(4, 3, 4, 3.5)
plot(x1, x2, col = colors, xlim = c(0, 5), ylim = c(0, 5))
abline(-0.8, 1)
plot(x1, x2, col = colors, xlim = c(0, 5), ylim = c(0, 5))
points(c(4), c(2), col = c("red"))
library(ISLR)
states=row.names(USArrests )
states
names(USArrests )
apply(USArrests , 2, mean)
apply(USArrests , 2, var)
pr.out=prcomp(USArrests , scale=TRUE)
names(pr.out)
pr.out$center
pr.out$rotation
dim(pr.out$x)
biplot(pr.out , scale=0)
pr.out$rotation=-pr.out$rotation
pr.out$x=-pr.out$x
biplot(pr.out , scale=0)
pr.out$sdev
pr.var=pr.out$sdev ^2
pr.var
pve=pr.var/sum(pr.var)
pve
plot(pve , xlab="Principal Component ", ylab="Proportion of Variance Explained ", ylim=c(0,1),type='b')
plot(cumsum(pve), xlab="Principal Component ", ylab=" Cumulative Proportion of Variance Explained ", ylim=c(0,1), type='b')
plot(pve , xlab="Principal Component ", ylab="Proportion of Variance Explained ", ylim=c(0,1),type='b')
plot(cumsum(pve), xlab="Principal Component ", ylab=" Cumulative Proportion of Variance Explained ", ylim=c(0,1), type='b')
a=c(1,2,8,-3)
cumsum(a)
set.seed(2)
x=matrix(rnorm (50*2), ncol=2)
x[1:25,1]=x[1:25,1]+3
x[1:25,2]=x[1:25,2]-4
km.out=kmeans(x,2,nstart =20)
km.out$cluster
plot(x, col=(km.out$cluster +1), main="K-Means Clustering Results with K=2", xlab="", ylab="", pch=20, cex=2)
set.seed(4)
km.out=kmeans(x,3,nstart =20)
km.out
set.seed(3)
km.out=kmeans(x,3,nstart =1)
km.out$tot .withinss
km.out$tot.withinss
km.out=kmeans(x,3,nstart =20)
km.out$tot.withinss
hc.complete =hclust(dist(x), method="complete ")
hc.complete =hclust(dist(x), method="complete")
hc.average =hclust(dist(x), method ="average ")
hc.single=hclust(dist(x), method ="single")
hc.average =hclust(dist(x), method ="average")
par(mfrow=c(1,3))
plot(hc.complete ,main="Complete Linkage", xlab="", sub="", cex=.9)
plot(hc.average , main="Average Linkage", xlab="", sub="", cex=.9)
plot(hc.single, main="Single Linkage ", xlab="", sub="", cex=.9)
cutree(hc.complete , 2) #[1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 [30] 222222222222222222222
cutree(hc.average , 2) #[1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 [30] 222122222222221212222
cutree(hc.single , 2)
cutree(hc.single , 4)
xsc=scale(x)
plot(hclust(dist(xsc), method ="complete"), main="Hierarchical Clustering with Scaled Features ")
x=matrix(rnorm (30*3), ncol=3)
dd=as.dist(1-cor(t(x)))
plot(hclust(dd, method ="complete"), main="Complete Linkage with Correlation -Based Distance ", xlab="", sub ="")
library(tidyverse) # Load the core tidyverse packages: ggplot2, tibble,
# tidyr, readr, purrr, and dplyr
library(vegan) #calculation of diversity metrics
library(psych) #descriptive statistics
library(class) #package for KNN model
library(randomForest) #package for Random Forest model
library(pscl) #For logistic regression R^2
library(ROCR) #For ROC curves
setwd("~/GitHub/DataMining_MicrobiomeProject")
microbiome <- read.csv("MicrobiomeWithMetadata.csv", encoding = 'utf-8', stringsAsFactors = FALSE)
library(psych)
nrow(microbiome) #675 observations
sum(is.na(microbiome)) #No missing data requiring imputation
colnames(microbiome[1:5])
#"Diet" "Source" "Donor" "CollectionMet" "Sex"
#Beyond this, all column names are OTU groups
table(microbiome$Donor)
#0   1   2   3   4   5   6
#313  64   6 151  92  46   3
#   Donor
# 0 - HMouseLFPP -- mice
# 1 - CONVR
# 2 - Human
# 3 - Fresh -- mice
# 4 - Frozen -- mice
# 5 - HMouseWestern -- mice
# 6 - CONVD
#We will be using only non-human (mouse) donors for our analysis
microbiome <- subset(microbiome, Donor == 0 | Donor == 3 | Donor == 4 | Donor == 5)
nrow(microbiome) #602 observations
#Further explore the distribution of these variables
table(microbiome$Diet)
# 0   1   4
# 350 243 9
#   Diet
#0	LFPP: low fat, high-plant polysaccharide diet
#1	Western: high-fat, high-sugar Western diet
#4	Suckling
#Remove all rows with suckling diets (want to compare only high and low fat diets)
microbiome <- subset(microbiome, Diet == 0 | Diet == 1)
nrow(microbiome) #593 -- FINAL NUMBER OF ROWS
#We will be using only Diet, Source, Sex, and OTU groups going forward
microbiome <- microbiome[,c(1,2,5:100)]
#Check for class types of each non-OTU column
class((microbiome[,1])) #integer
class((microbiome[,2])) #integer
class((microbiome[,3])) #integer
#All of these should be factors
microbiome[,1] <- as.factor(microbiome[,1])
microbiome[,2] <- as.factor(microbiome[,2])
microbiome[,3] <- as.factor(microbiome[,3])
#Double check for correct conversion
class((microbiome[,1])) #factor - good
table(microbiome$Sex)
#  0   1
# 543  50  -- Uneven distribution between males and females (see below for reference)
#Sex
#0 - Male
#1 - Female
plot(Diet~Sex, data=microbiome)
#Females appear to be more unevenly split between diets (favoring the low fat diet)
#   Diet
#0	LFPP: low fat, high-plant polysaccharide diet
#1	Western: high-fat, high-sugar Western diet
table(microbiome$Source)
#  0   1   2   3   4   5   6   7   8   9  10  11  12
# 13  13  15  10 444  10  12  12   9  13   9   7  26
plot(Diet~Source, data=microbiome)
#11 is interesting, as it shows that samples from the stomach are classified as high fat only
#This is likely a coincidence
#Sources
# 0 - Cecum1
# 1 - Cecum2
# 2 - Colon1
# 3 - Colon2
# 4 - Feces
# 5 - SI1
# 6 - SI13
# 7 - SI15
# 8 - SI2
# 9 - SI5
# 10 - SI9
# 11 - Stomach
# 12 - Cecum
microbiome$ShannonIndex <- NULL
microbiome$ShannonIndex <- diversity(microbiome[,4:98])
summary(microbiome$ShannonIndex)
#Min.  1st Qu.   Median     Mean  3rd Qu.     Max.
#0.000001 0.348860 0.693277 1.213849 1.168294 4.432951
#Reyyi Index
#specnumber fucntion finds the number of species in the sample
specnumber(microbiome[,4:98]) #95 species present in each sample
h <- diversity(microbiome[,4:98])
J <- h/log(specnumber(microbiome[,4:98])) #assessing Pielou's eveness
#now take a random subset of six sites to assess Renyi diversity index
set.seed(2)
k <- sample(nrow(microbiome[,4:98]), 6)
R <- renyi(microbiome[,4:98][k,])
plot(R) #can visualize these six sites, label sites chosen
#also make new column for Renyi index
microbiome$Renyi <- NULL
microbiome$Renyi <- renyi(microbiome[,4:98], scales = 32)
summary(microbiome$Renyi)
# Min.  1st Qu.   Median   Mean  3rd Qu.     Max.
# 0.0000  0.1216  0.5273  0.9932  0.9235  4.0791
#Is there a significant difference between these indexes?
comp.index <- aov(microbiome$ShannonIndex ~ microbiome$Renyi, data=microbiome)
plot(comp.index)
train <- microbiome[train, c(1:98)]
microbiome <- microbiome[, c(1:98)]
set.seed(17)
train = sample(1:nrow(microbiome), nrow(microbiome) * .75)
test <- microbiome[-train, c(1:98)]
train <- microbiome[train, c(1:98)]
microbiome <- microbiome[, c(1:98)]
library(randomForest)
library(randomForest) #package for Random Forest model
library (gbm) #package for boosting model
library(pscl) #For logistic regression R^2
library(ROCR) #For ROC curves
boost.biome <- gbm(Diet~., data = train, distribution = 'bernoulli', n.trees = 5000)
boost.biome <- gbm(Diet~., data = train, distribution = 'huberized', n.trees = 5000)
boost.biome
par(mfrow=c(1,2))
plot(boost.biome ,i="rm")
summary(boost.biome)
boost.biome2 <- gbm(Diet~., data = train, distribution = 'bernoulli', n.trees = 5000)
importance(boost.biome)
boost.biome <- gbm(Diet~., data = train, distribution = 'huberized', n.trees = 5000, shrinkage = .01)
boost.biome
summary(boost.biome)
importance(boost.biome)
library(ROCR) #For ROC curves
set.seed(100)
rf.biome= randomForest(Diet~.,data=train, mtry=80, importance =TRUE)
rf.biome
importance (rf.biome)
varImpPlot (rf.biome)
yhat.rf = predict(rf.biome,newdata=test, type="response")
biome.pred <- predict(rf.biome, type="response")
train$rf <- biome.pred
rf.roc <- roc(Diet ~ rf, data = train)
library(ROCR) #For ROC curves
library(pROC) #ROC curves
rf.roc <- roc(Diet ~ rf, data = train)
View(train)
View(train)
nrow(microbiome) #675 observations
sum(is.na(microbiome)) #No missing data requiring imputation
colnames(microbiome[1:5])
microbiome <- subset(microbiome, Donor == 0 | Donor == 3 | Donor == 4 | Donor == 5)
nrow(microbiome) #602 observations
table(microbiome$Diet)
microbiome[,1] <- as.factor(microbiome[,1])
microbiome[,2] <- as.factor(microbiome[,2])
microbiome[,3] <- as.factor(microbiome[,3])
biome.pred <- predict(rf.biome, type="response")
train$rf <- biome.pred
rf.roc <- roc(Diet ~ rf, data = train)
library(caret)
fitControl <- trainControl(method = "cv", number = 10 ) #5folds)
tune_Grid <-  expand.grid(interaction.depth = 2, n.trees = 2500, shrinkage = 0.01, n.minobsinnode = 10)
set.seed(108)
fit <- train(Diet ~ ., data = train,method = "gbm", trControl = fitControl, verbose = FALSE, tuneGrid = gbmGrid)
fit <- train(Diet ~ ., data = train,method = "gbm", trControl = fitControl, verbose = FALSE, tuneGrid = tune_Grid)
summary(fit)
fit
importance(fit)
plot(boost.biome ,i="rf1") to plot against significant regressors
plot(boost.biome ,i= 'rf1') to plot against significant regressors
plot(fit ,i= 'rf1') to plot against significant regressors
plot(fit ,i= 'rf1') #to plot against significant regressors
plot(fit ,i= 'OTU41 ') #to plot against significant regressors
plot(fit ,i= 'OTU41 ') #to plot against significant regressors
varImpPlot (rf.biome)
plot(rf.roc)
biome.pred <- predict(rf.biome, type="response")
train$rf <- biome.pred
rf.roc <- roc(Diet ~ rf, data = train)
plot(rf.roc)
varImpPlot (fit)
fit.pred <- predict(fit, type="response")
